Device cuda
Loading csv... done, took 5.8s
Parsing... done, took 4.9s
Entropy of DMV([Column(Record Type, distribution_size=2), Column(Registration Class, distribution_size=69), Column(State, distribution_size=77), Column(County, distribution_size=63), Column(Body Type, distribution_size=57), Column(Fuel Type, distribution_size=8), Column(Reg Valid Date, distribution_size=2828), Column(Color, distribution_size=218), Column(Scofflaw Indicator, distribution_size=2), Column(Suspension Indicator, distribution_size=2), Column(Revocation Indicator, distribution_size=2)]): 19.3435 bits
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 7315785 entries, 0 to 7315784
Data columns (total 11 columns):
 #   Column                Dtype         
---  ------                -----         
 0   Record Type           object        
 1   Registration Class    object        
 2   State                 object        
 3   County                object        
 4   Body Type             object        
 5   Fuel Type             object        
 6   Reg Valid Date        datetime64[ns]
 7   Color                 object        
 8   Scofflaw Indicator    object        
 9   Suspension Indicator  object        
 10  Revocation Indicator  object        
dtypes: datetime64[ns](1), object(10)
memory usage: 614.0+ MB
None
fixed_ordering None seed 0 natural_ordering True
encoded_bins (output) [2, 69, 77, 63, 57, 8, 2828, 218, 2, 2, 2]
encoded_bins (input) [1, 7, 7, 6, 6, 3, 12, 8, 1, 1, 1]
Number of model parameters: 1575168 (~= 6.0MB)
MADE(
  (net): Sequential(
    (0): MaskedLinear(in_features=53, out_features=256, bias=True)
    (1): MaskedResidualBlock(
      (layers): ModuleList(
        (0): MaskedLinear(in_features=256, out_features=256, bias=True)
        (1): MaskedLinear(in_features=256, out_features=256, bias=True)
      )
      (activation): ReLU()
    )
    (2): MaskedResidualBlock(
      (layers): ModuleList(
        (0): MaskedLinear(in_features=256, out_features=256, bias=True)
        (1): MaskedLinear(in_features=256, out_features=256, bias=True)
      )
      (activation): ReLU()
    )
    (3): MaskedResidualBlock(
      (layers): ModuleList(
        (0): MaskedLinear(in_features=256, out_features=256, bias=True)
        (1): MaskedLinear(in_features=256, out_features=256, bias=True)
      )
      (activation): ReLU()
    )
    (4): MaskedResidualBlock(
      (layers): ModuleList(
        (0): MaskedLinear(in_features=256, out_features=256, bias=True)
        (1): MaskedLinear(in_features=256, out_features=256, bias=True)
      )
      (activation): ReLU()
    )
    (5): MaskedLinear(in_features=256, out_features=3328, bias=True)
  )
  (direct_io_layer): MaskedLinear(in_features=53, out_features=3328, bias=True)
)
Applying InitWeight()
Discretizing table... done, took 3.8s
Device cuda
Loading csv... done, took 5.9s
Parsing... done, took 5.0s
Entropy of DMV([Column(Record Type, distribution_size=2), Column(Registration Class, distribution_size=69), Column(State, distribution_size=77), Column(County, distribution_size=63), Column(Body Type, distribution_size=57), Column(Fuel Type, distribution_size=8), Column(Reg Valid Date, distribution_size=2828), Column(Color, distribution_size=218), Column(Scofflaw Indicator, distribution_size=2), Column(Suspension Indicator, distribution_size=2), Column(Revocation Indicator, distribution_size=2)]): 19.3435 bits
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 7315785 entries, 0 to 7315784
Data columns (total 11 columns):
 #   Column                Dtype         
---  ------                -----         
 0   Record Type           object        
 1   Registration Class    object        
 2   State                 object        
 3   County                object        
 4   Body Type             object        
 5   Fuel Type             object        
 6   Reg Valid Date        datetime64[ns]
 7   Color                 object        
 8   Scofflaw Indicator    object        
 9   Suspension Indicator  object        
 10  Revocation Indicator  object        
dtypes: datetime64[ns](1), object(10)
memory usage: 614.0+ MB
None
fixed_ordering None seed 0 natural_ordering True
encoded_bins (output) [2, 69, 77, 63, 57, 8, 2828, 218, 2, 2, 2]
encoded_bins (input) [1, 7, 7, 6, 6, 3, 12, 8, 1, 1, 1]
Number of model parameters: 1575168 (~= 6.0MB)
MADE(
  (net): Sequential(
    (0): MaskedLinear(in_features=53, out_features=256, bias=True)
    (1): MaskedResidualBlock(
      (layers): ModuleList(
        (0): MaskedLinear(in_features=256, out_features=256, bias=True)
        (1): MaskedLinear(in_features=256, out_features=256, bias=True)
      )
      (activation): ReLU()
    )
    (2): MaskedResidualBlock(
      (layers): ModuleList(
        (0): MaskedLinear(in_features=256, out_features=256, bias=True)
        (1): MaskedLinear(in_features=256, out_features=256, bias=True)
      )
      (activation): ReLU()
    )
    (3): MaskedResidualBlock(
      (layers): ModuleList(
        (0): MaskedLinear(in_features=256, out_features=256, bias=True)
        (1): MaskedLinear(in_features=256, out_features=256, bias=True)
      )
      (activation): ReLU()
    )
    (4): MaskedResidualBlock(
      (layers): ModuleList(
        (0): MaskedLinear(in_features=256, out_features=256, bias=True)
        (1): MaskedLinear(in_features=256, out_features=256, bias=True)
      )
      (activation): ReLU()
    )
    (5): MaskedLinear(in_features=256, out_features=3328, bias=True)
  )
  (direct_io_layer): MaskedLinear(in_features=53, out_features=3328, bias=True)
)
Applying InitWeight()
Discretizing table... done, took 3.6s
Epoch 0 Iter 0, train entropy gap 31.0318 bits (loss 50.375, data 19.343) 
Epoch 0 Iter 200, train entropy gap 30.4474 bits (loss 49.791, data 19.343) 
Epoch 0 Iter 400, train entropy gap 28.6190 bits (loss 47.962, data 19.343) 
Epoch 0 Iter 600, train entropy gap 24.3598 bits (loss 43.703, data 19.343) 
Epoch 0 Iter 800, train entropy gap 16.4907 bits (loss 35.834, data 19.343) 
Epoch 0 Iter 1000, train entropy gap 12.6329 bits (loss 31.976, data 19.343) 
Epoch 0 Iter 1200, train entropy gap 8.3985 bits (loss 27.742, data 19.343) 
Epoch 0 Iter 1400, train entropy gap 5.2701 bits (loss 24.614, data 19.343) 
Epoch 0 Iter 1600, train entropy gap 3.7759 bits (loss 23.119, data 19.343) 
Epoch 0 Iter 1800, train entropy gap 3.6522 bits (loss 22.996, data 19.343) 
Epoch 0 Iter 2000, train entropy gap 3.5821 bits (loss 22.926, data 19.343) 
Epoch 0 Iter 2200, train entropy gap 3.1733 bits (loss 22.517, data 19.343) 
Epoch 0 Iter 2400, train entropy gap 2.6917 bits (loss 22.035, data 19.343) 
Epoch 0 Iter 2600, train entropy gap 2.8051 bits (loss 22.149, data 19.343) 
Epoch 0 Iter 2800, train entropy gap 2.6110 bits (loss 21.954, data 19.343) 
Epoch 0 Iter 3000, train entropy gap 2.4681 bits (loss 21.812, data 19.343) 
Epoch 0 Iter 3200, train entropy gap 2.3538 bits (loss 21.697, data 19.343) 
Epoch 0 Iter 3400, train entropy gap 2.2825 bits (loss 21.626, data 19.343) 
epoch 0 train loss 20.0839 nats / 28.9749 bits
time since start: 31.9 secs
Epoch 1 Iter 0, train entropy gap 2.2066 bits (loss 21.550, data 19.343) 
Epoch 1 Iter 200, train entropy gap 2.0484 bits (loss 21.392, data 19.343) 
Epoch 1 Iter 400, train entropy gap 2.0516 bits (loss 21.395, data 19.343) 
Epoch 1 Iter 600, train entropy gap 2.1848 bits (loss 21.528, data 19.343) 
Epoch 1 Iter 800, train entropy gap 2.1006 bits (loss 21.444, data 19.343) 
Epoch 1 Iter 1000, train entropy gap 1.8971 bits (loss 21.241, data 19.343) 
Epoch 1 Iter 1200, train entropy gap 2.1349 bits (loss 21.478, data 19.343) 
Epoch 1 Iter 1400, train entropy gap 1.9996 bits (loss 21.343, data 19.343) 
Epoch 1 Iter 1600, train entropy gap 2.0356 bits (loss 21.379, data 19.343) 
Epoch 1 Iter 1800, train entropy gap 1.9545 bits (loss 21.298, data 19.343) 
Epoch 1 Iter 2000, train entropy gap 2.0489 bits (loss 21.392, data 19.343) 
Epoch 1 Iter 2200, train entropy gap 2.1945 bits (loss 21.538, data 19.343) 
Epoch 1 Iter 2400, train entropy gap 1.7025 bits (loss 21.046, data 19.343) 
Epoch 1 Iter 2600, train entropy gap 1.6786 bits (loss 21.022, data 19.343) 
Epoch 1 Iter 2800, train entropy gap 1.5821 bits (loss 20.926, data 19.343) 
Epoch 1 Iter 3000, train entropy gap 1.3554 bits (loss 20.699, data 19.343) 
Epoch 1 Iter 3200, train entropy gap 1.5241 bits (loss 20.868, data 19.343) 
Epoch 1 Iter 3400, train entropy gap 1.4747 bits (loss 20.818, data 19.343) 
epoch 1 train loss 14.7221 nats / 21.2395 bits
time since start: 63.2 secs
Epoch 2 Iter 0, train entropy gap 1.4432 bits (loss 20.787, data 19.343) 
Epoch 2 Iter 200, train entropy gap 1.2969 bits (loss 20.640, data 19.343) 
Epoch 2 Iter 400, train entropy gap 1.3455 bits (loss 20.689, data 19.343) 
Epoch 2 Iter 600, train entropy gap 1.2727 bits (loss 20.616, data 19.343) 
Epoch 2 Iter 800, train entropy gap 1.3264 bits (loss 20.670, data 19.343) 
Epoch 2 Iter 1000, train entropy gap 1.4191 bits (loss 20.763, data 19.343) 
Epoch 2 Iter 1200, train entropy gap 1.3762 bits (loss 20.720, data 19.343) 
Epoch 2 Iter 1400, train entropy gap 1.5406 bits (loss 20.884, data 19.343) 
Epoch 2 Iter 1600, train entropy gap 1.5681 bits (loss 20.912, data 19.343) 
Epoch 2 Iter 1800, train entropy gap 1.4533 bits (loss 20.797, data 19.343) 
Epoch 2 Iter 2000, train entropy gap 1.5037 bits (loss 20.847, data 19.343) 
Epoch 2 Iter 2200, train entropy gap 1.5065 bits (loss 20.850, data 19.343) 
Epoch 2 Iter 2400, train entropy gap 1.3069 bits (loss 20.650, data 19.343) 
Epoch 2 Iter 2600, train entropy gap 1.2818 bits (loss 20.625, data 19.343) 
Epoch 2 Iter 2800, train entropy gap 1.3025 bits (loss 20.646, data 19.343) 
Epoch 2 Iter 3000, train entropy gap 1.2040 bits (loss 20.547, data 19.343) 
Epoch 2 Iter 3200, train entropy gap 1.3806 bits (loss 20.724, data 19.343) 
Epoch 2 Iter 3400, train entropy gap 1.2746 bits (loss 20.618, data 19.343) 
epoch 2 train loss 14.3439 nats / 20.6938 bits
time since start: 94.3 secs
Epoch 3 Iter 0, train entropy gap 1.1847 bits (loss 20.528, data 19.343) 
Epoch 3 Iter 200, train entropy gap 1.0410 bits (loss 20.384, data 19.343) 
Epoch 3 Iter 400, train entropy gap 1.2679 bits (loss 20.611, data 19.343) 
Epoch 3 Iter 600, train entropy gap 1.2754 bits (loss 20.619, data 19.343) 
Epoch 3 Iter 800, train entropy gap 1.1992 bits (loss 20.543, data 19.343) 
Epoch 3 Iter 1000, train entropy gap 1.1717 bits (loss 20.515, data 19.343) 
Epoch 3 Iter 1200, train entropy gap 1.0526 bits (loss 20.396, data 19.343) 
Epoch 3 Iter 1400, train entropy gap 1.1575 bits (loss 20.501, data 19.343) 
Epoch 3 Iter 1600, train entropy gap 1.2234 bits (loss 20.567, data 19.343) 
Epoch 3 Iter 1800, train entropy gap 1.0755 bits (loss 20.419, data 19.343) 
Epoch 3 Iter 2000, train entropy gap 1.1692 bits (loss 20.513, data 19.343) 
Epoch 3 Iter 2200, train entropy gap 1.2235 bits (loss 20.567, data 19.343) 
Epoch 3 Iter 2400, train entropy gap 1.1072 bits (loss 20.451, data 19.343) 
Epoch 3 Iter 2600, train entropy gap 1.1629 bits (loss 20.506, data 19.343) 
Epoch 3 Iter 2800, train entropy gap 1.2003 bits (loss 20.544, data 19.343) 
Epoch 3 Iter 3000, train entropy gap 1.1671 bits (loss 20.511, data 19.343) 
Epoch 3 Iter 3200, train entropy gap 1.1741 bits (loss 20.518, data 19.343) 
Epoch 3 Iter 3400, train entropy gap 1.2874 bits (loss 20.631, data 19.343) 
epoch 3 train loss 14.2290 nats / 20.5281 bits
time since start: 125.5 secs
Epoch 4 Iter 0, train entropy gap 1.0994 bits (loss 20.443, data 19.343) 
Epoch 4 Iter 200, train entropy gap 1.0575 bits (loss 20.401, data 19.343) 
Epoch 4 Iter 400, train entropy gap 1.0905 bits (loss 20.434, data 19.343) 
Epoch 4 Iter 600, train entropy gap 1.2661 bits (loss 20.610, data 19.343) 
Epoch 4 Iter 800, train entropy gap 1.0253 bits (loss 20.369, data 19.343) 
Epoch 4 Iter 1000, train entropy gap 1.3405 bits (loss 20.684, data 19.343) 
Epoch 4 Iter 1200, train entropy gap 0.9807 bits (loss 20.324, data 19.343) 
Epoch 4 Iter 1400, train entropy gap 1.0035 bits (loss 20.347, data 19.343) 
Epoch 4 Iter 1600, train entropy gap 1.0886 bits (loss 20.432, data 19.343) 
Epoch 4 Iter 1800, train entropy gap 1.1770 bits (loss 20.521, data 19.343) 
Epoch 4 Iter 2000, train entropy gap 1.1283 bits (loss 20.472, data 19.343) 
Epoch 4 Iter 2200, train entropy gap 0.9337 bits (loss 20.277, data 19.343) 
Epoch 4 Iter 2400, train entropy gap 1.1922 bits (loss 20.536, data 19.343) 
Epoch 4 Iter 2600, train entropy gap 1.1491 bits (loss 20.493, data 19.343) 
Epoch 4 Iter 2800, train entropy gap 1.0316 bits (loss 20.375, data 19.343) 
Epoch 4 Iter 3000, train entropy gap 0.9899 bits (loss 20.333, data 19.343) 
Epoch 4 Iter 3200, train entropy gap 0.9651 bits (loss 20.309, data 19.343) 
Epoch 4 Iter 3400, train entropy gap 1.0694 bits (loss 20.413, data 19.343) 
epoch 4 train loss 14.1710 nats / 20.4445 bits
time since start: 157.1 secs
Epoch 5 Iter 0, train entropy gap 0.9596 bits (loss 20.303, data 19.343) 
Epoch 5 Iter 200, train entropy gap 0.9045 bits (loss 20.248, data 19.343) 
Epoch 5 Iter 400, train entropy gap 1.0141 bits (loss 20.358, data 19.343) 
Epoch 5 Iter 600, train entropy gap 1.2421 bits (loss 20.586, data 19.343) 
Epoch 5 Iter 800, train entropy gap 1.1210 bits (loss 20.464, data 19.343) 
Epoch 5 Iter 1000, train entropy gap 1.2079 bits (loss 20.551, data 19.343) 
Epoch 5 Iter 1200, train entropy gap 1.0513 bits (loss 20.395, data 19.343) 
Epoch 5 Iter 1400, train entropy gap 1.1078 bits (loss 20.451, data 19.343) 
Epoch 5 Iter 1600, train entropy gap 1.1367 bits (loss 20.480, data 19.343) 
Epoch 5 Iter 1800, train entropy gap 0.9724 bits (loss 20.316, data 19.343) 
Epoch 5 Iter 2000, train entropy gap 1.1105 bits (loss 20.454, data 19.343) 
Epoch 5 Iter 2200, train entropy gap 0.9388 bits (loss 20.282, data 19.343) 
Epoch 5 Iter 2400, train entropy gap 0.9271 bits (loss 20.271, data 19.343) 
Epoch 5 Iter 2600, train entropy gap 1.0726 bits (loss 20.416, data 19.343) 
Epoch 5 Iter 2800, train entropy gap 0.9655 bits (loss 20.309, data 19.343) 
Epoch 5 Iter 3000, train entropy gap 1.1235 bits (loss 20.467, data 19.343) 
Epoch 5 Iter 3200, train entropy gap 1.0450 bits (loss 20.388, data 19.343) 
Epoch 5 Iter 3400, train entropy gap 1.1140 bits (loss 20.457, data 19.343) 
epoch 5 train loss 14.1426 nats / 20.4034 bits
time since start: 188.8 secs
Epoch 6 Iter 0, train entropy gap 0.9444 bits (loss 20.288, data 19.343) 
Epoch 6 Iter 200, train entropy gap 0.9697 bits (loss 20.313, data 19.343) 
Epoch 6 Iter 400, train entropy gap 1.0249 bits (loss 20.368, data 19.343) 
Epoch 6 Iter 600, train entropy gap 0.9179 bits (loss 20.261, data 19.343) 
Epoch 6 Iter 800, train entropy gap 1.0060 bits (loss 20.349, data 19.343) 
Epoch 6 Iter 1000, train entropy gap 1.1619 bits (loss 20.505, data 19.343) 
Epoch 6 Iter 1200, train entropy gap 0.8523 bits (loss 20.196, data 19.343) 
Epoch 6 Iter 1400, train entropy gap 1.0011 bits (loss 20.345, data 19.343) 
Epoch 6 Iter 1600, train entropy gap 1.0608 bits (loss 20.404, data 19.343) 
Epoch 6 Iter 1800, train entropy gap 0.9855 bits (loss 20.329, data 19.343) 
Epoch 6 Iter 2000, train entropy gap 1.1300 bits (loss 20.473, data 19.343) 
Epoch 6 Iter 2200, train entropy gap 1.1859 bits (loss 20.529, data 19.343) 
Epoch 6 Iter 2400, train entropy gap 0.9029 bits (loss 20.246, data 19.343) 
Epoch 6 Iter 2600, train entropy gap 1.0987 bits (loss 20.442, data 19.343) 
Epoch 6 Iter 2800, train entropy gap 1.0164 bits (loss 20.360, data 19.343) 
Epoch 6 Iter 3000, train entropy gap 1.0043 bits (loss 20.348, data 19.343) 
Epoch 6 Iter 3200, train entropy gap 0.9077 bits (loss 20.251, data 19.343) 
Epoch 6 Iter 3400, train entropy gap 1.0663 bits (loss 20.410, data 19.343) 
epoch 6 train loss 14.1258 nats / 20.3792 bits
time since start: 220.1 secs
Epoch 7 Iter 0, train entropy gap 1.0430 bits (loss 20.386, data 19.343) 
Epoch 7 Iter 200, train entropy gap 0.8444 bits (loss 20.188, data 19.343) 
Epoch 7 Iter 400, train entropy gap 0.8241 bits (loss 20.168, data 19.343) 
Epoch 7 Iter 600, train entropy gap 1.0775 bits (loss 20.421, data 19.343) 
Epoch 7 Iter 800, train entropy gap 0.9712 bits (loss 20.315, data 19.343) 
Epoch 7 Iter 1000, train entropy gap 1.0057 bits (loss 20.349, data 19.343) 
Epoch 7 Iter 1200, train entropy gap 0.8347 bits (loss 20.178, data 19.343) 
Epoch 7 Iter 1400, train entropy gap 0.9392 bits (loss 20.283, data 19.343) 
Epoch 7 Iter 1600, train entropy gap 0.9999 bits (loss 20.343, data 19.343) 
Epoch 7 Iter 1800, train entropy gap 0.9401 bits (loss 20.284, data 19.343) 
Epoch 7 Iter 2000, train entropy gap 0.9602 bits (loss 20.304, data 19.343) 
Epoch 7 Iter 2200, train entropy gap 0.9977 bits (loss 20.341, data 19.343) 
Epoch 7 Iter 2400, train entropy gap 0.9691 bits (loss 20.313, data 19.343) 
Epoch 7 Iter 2600, train entropy gap 0.9783 bits (loss 20.322, data 19.343) 
Epoch 7 Iter 2800, train entropy gap 1.0695 bits (loss 20.413, data 19.343) 
Epoch 7 Iter 3000, train entropy gap 0.9243 bits (loss 20.268, data 19.343) 
Epoch 7 Iter 3200, train entropy gap 1.0261 bits (loss 20.370, data 19.343) 
Epoch 7 Iter 3400, train entropy gap 0.9575 bits (loss 20.301, data 19.343) 
epoch 7 train loss 14.1129 nats / 20.3605 bits
time since start: 251.4 secs
Epoch 8 Iter 0, train entropy gap 1.0365 bits (loss 20.380, data 19.343) 
Epoch 8 Iter 200, train entropy gap 1.0459 bits (loss 20.389, data 19.343) 
Epoch 8 Iter 400, train entropy gap 0.9640 bits (loss 20.307, data 19.343) 
Epoch 8 Iter 600, train entropy gap 0.8970 bits (loss 20.240, data 19.343) 
Epoch 8 Iter 800, train entropy gap 1.0280 bits (loss 20.371, data 19.343) 
Epoch 8 Iter 1000, train entropy gap 1.0236 bits (loss 20.367, data 19.343) 
Epoch 8 Iter 1200, train entropy gap 0.9560 bits (loss 20.299, data 19.343) 
Epoch 8 Iter 1400, train entropy gap 1.0152 bits (loss 20.359, data 19.343) 
Epoch 8 Iter 1600, train entropy gap 0.8941 bits (loss 20.238, data 19.343) 
Epoch 8 Iter 1800, train entropy gap 0.9233 bits (loss 20.267, data 19.343) 
Epoch 8 Iter 2000, train entropy gap 1.0529 bits (loss 20.396, data 19.343) 
Epoch 8 Iter 2200, train entropy gap 0.9868 bits (loss 20.330, data 19.343) 
Epoch 8 Iter 2400, train entropy gap 0.9325 bits (loss 20.276, data 19.343) 
Epoch 8 Iter 2600, train entropy gap 1.0974 bits (loss 20.441, data 19.343) 
Epoch 8 Iter 2800, train entropy gap 0.9654 bits (loss 20.309, data 19.343) 
Epoch 8 Iter 3000, train entropy gap 1.0642 bits (loss 20.408, data 19.343) 
Epoch 8 Iter 3200, train entropy gap 1.0372 bits (loss 20.381, data 19.343) 
Epoch 8 Iter 3400, train entropy gap 1.0357 bits (loss 20.379, data 19.343) 
epoch 8 train loss 14.1027 nats / 20.3458 bits
time since start: 282.8 secs
Epoch 9 Iter 0, train entropy gap 0.9987 bits (loss 20.342, data 19.343) 
Epoch 9 Iter 200, train entropy gap 1.0247 bits (loss 20.368, data 19.343) 
Epoch 9 Iter 400, train entropy gap 0.8985 bits (loss 20.242, data 19.343) 
Epoch 9 Iter 600, train entropy gap 0.9664 bits (loss 20.310, data 19.343) 
Epoch 9 Iter 800, train entropy gap 1.0259 bits (loss 20.369, data 19.343) 
Epoch 9 Iter 1000, train entropy gap 1.0502 bits (loss 20.394, data 19.343) 
Epoch 9 Iter 1200, train entropy gap 0.9276 bits (loss 20.271, data 19.343) 
Epoch 9 Iter 1400, train entropy gap 1.0002 bits (loss 20.344, data 19.343) 
Epoch 9 Iter 1600, train entropy gap 0.9338 bits (loss 20.277, data 19.343) 
Epoch 9 Iter 1800, train entropy gap 0.9676 bits (loss 20.311, data 19.343) 
Epoch 9 Iter 2000, train entropy gap 0.8838 bits (loss 20.227, data 19.343) 
Epoch 9 Iter 2200, train entropy gap 0.9713 bits (loss 20.315, data 19.343) 
Epoch 9 Iter 2400, train entropy gap 0.9939 bits (loss 20.337, data 19.343) 
Epoch 9 Iter 2600, train entropy gap 1.0857 bits (loss 20.429, data 19.343) 
Epoch 9 Iter 2800, train entropy gap 0.9444 bits (loss 20.288, data 19.343) 
Epoch 9 Iter 3000, train entropy gap 0.9379 bits (loss 20.281, data 19.343) 
Epoch 9 Iter 3200, train entropy gap 0.8327 bits (loss 20.176, data 19.343) 
Epoch 9 Iter 3400, train entropy gap 1.0290 bits (loss 20.372, data 19.343) 
epoch 9 train loss 14.0941 nats / 20.3335 bits
time since start: 314.1 secs
Epoch 10 Iter 0, train entropy gap 0.9969 bits (loss 20.340, data 19.343) 
Epoch 10 Iter 200, train entropy gap 0.9216 bits (loss 20.265, data 19.343) 
Epoch 10 Iter 400, train entropy gap 0.9879 bits (loss 20.331, data 19.343) 
Epoch 10 Iter 600, train entropy gap 1.0209 bits (loss 20.364, data 19.343) 
Epoch 10 Iter 800, train entropy gap 1.1641 bits (loss 20.508, data 19.343) 
Epoch 10 Iter 1000, train entropy gap 0.9281 bits (loss 20.272, data 19.343) 
Epoch 10 Iter 1200, train entropy gap 0.9593 bits (loss 20.303, data 19.343) 
Epoch 10 Iter 1400, train entropy gap 0.9803 bits (loss 20.324, data 19.343) 
Epoch 10 Iter 1600, train entropy gap 0.8651 bits (loss 20.209, data 19.343) 
Epoch 10 Iter 1800, train entropy gap 0.9696 bits (loss 20.313, data 19.343) 
Epoch 10 Iter 2000, train entropy gap 1.0966 bits (loss 20.440, data 19.343) 
Epoch 10 Iter 2200, train entropy gap 1.0595 bits (loss 20.403, data 19.343) 
Epoch 10 Iter 2400, train entropy gap 0.9691 bits (loss 20.313, data 19.343) 
Epoch 10 Iter 2600, train entropy gap 1.1545 bits (loss 20.498, data 19.343) 
Epoch 10 Iter 2800, train entropy gap 0.9000 bits (loss 20.243, data 19.343) 
Epoch 10 Iter 3000, train entropy gap 0.9575 bits (loss 20.301, data 19.343) 
Epoch 10 Iter 3200, train entropy gap 0.9712 bits (loss 20.315, data 19.343) 
Epoch 10 Iter 3400, train entropy gap 1.0676 bits (loss 20.411, data 19.343) 
epoch 10 train loss 14.0868 nats / 20.3230 bits
time since start: 345.4 secs
Epoch 11 Iter 0, train entropy gap 1.0058 bits (loss 20.349, data 19.343) 
Epoch 11 Iter 200, train entropy gap 1.1122 bits (loss 20.456, data 19.343) 
Epoch 11 Iter 400, train entropy gap 1.0108 bits (loss 20.354, data 19.343) 
Epoch 11 Iter 600, train entropy gap 0.9849 bits (loss 20.328, data 19.343) 
Epoch 11 Iter 800, train entropy gap 1.1136 bits (loss 20.457, data 19.343) 
Epoch 11 Iter 1000, train entropy gap 0.9834 bits (loss 20.327, data 19.343) 
Epoch 11 Iter 1200, train entropy gap 0.9639 bits (loss 20.307, data 19.343) 
Epoch 11 Iter 1400, train entropy gap 1.0576 bits (loss 20.401, data 19.343) 
Epoch 11 Iter 1600, train entropy gap 0.9702 bits (loss 20.314, data 19.343) 
Epoch 11 Iter 1800, train entropy gap 0.9364 bits (loss 20.280, data 19.343) 
Epoch 11 Iter 2000, train entropy gap 1.1696 bits (loss 20.513, data 19.343) 
Epoch 11 Iter 2200, train entropy gap 0.8763 bits (loss 20.220, data 19.343) 
Epoch 11 Iter 2400, train entropy gap 0.9468 bits (loss 20.290, data 19.343) 
Epoch 11 Iter 2600, train entropy gap 0.9122 bits (loss 20.256, data 19.343) 
Epoch 11 Iter 2800, train entropy gap 1.0050 bits (loss 20.348, data 19.343) 
Epoch 11 Iter 3000, train entropy gap 0.9535 bits (loss 20.297, data 19.343) 
Epoch 11 Iter 3200, train entropy gap 0.7837 bits (loss 20.127, data 19.343) 
Epoch 11 Iter 3400, train entropy gap 1.0417 bits (loss 20.385, data 19.343) 
epoch 11 train loss 14.0806 nats / 20.3140 bits
time since start: 376.6 secs
Epoch 12 Iter 0, train entropy gap 0.9107 bits (loss 20.254, data 19.343) 
Epoch 12 Iter 200, train entropy gap 0.9651 bits (loss 20.309, data 19.343) 
Epoch 12 Iter 400, train entropy gap 0.8265 bits (loss 20.170, data 19.343) 
Epoch 12 Iter 600, train entropy gap 0.9895 bits (loss 20.333, data 19.343) 
Epoch 12 Iter 800, train entropy gap 1.0626 bits (loss 20.406, data 19.343) 
Epoch 12 Iter 1000, train entropy gap 0.9910 bits (loss 20.334, data 19.343) 
Epoch 12 Iter 1200, train entropy gap 1.0565 bits (loss 20.400, data 19.343) 
Epoch 12 Iter 1400, train entropy gap 0.9867 bits (loss 20.330, data 19.343) 
Epoch 12 Iter 1600, train entropy gap 0.9300 bits (loss 20.273, data 19.343) 
Epoch 12 Iter 1800, train entropy gap 0.9456 bits (loss 20.289, data 19.343) 
Epoch 12 Iter 2000, train entropy gap 1.2043 bits (loss 20.548, data 19.343) 
Epoch 12 Iter 2200, train entropy gap 0.8223 bits (loss 20.166, data 19.343) 
Epoch 12 Iter 2400, train entropy gap 0.9680 bits (loss 20.311, data 19.343) 
Epoch 12 Iter 2600, train entropy gap 0.9195 bits (loss 20.263, data 19.343) 
Epoch 12 Iter 2800, train entropy gap 0.8630 bits (loss 20.206, data 19.343) 
Epoch 12 Iter 3000, train entropy gap 0.9468 bits (loss 20.290, data 19.343) 
Epoch 12 Iter 3200, train entropy gap 0.9653 bits (loss 20.309, data 19.343) 
Epoch 12 Iter 3400, train entropy gap 0.9928 bits (loss 20.336, data 19.343) 
epoch 12 train loss 14.0749 nats / 20.3058 bits
time since start: 408.0 secs
Epoch 13 Iter 0, train entropy gap 0.8230 bits (loss 20.166, data 19.343) 
Epoch 13 Iter 200, train entropy gap 1.0658 bits (loss 20.409, data 19.343) 
Epoch 13 Iter 400, train entropy gap 0.9139 bits (loss 20.257, data 19.343) 
Epoch 13 Iter 600, train entropy gap 0.9481 bits (loss 20.292, data 19.343) 
Epoch 13 Iter 800, train entropy gap 0.8562 bits (loss 20.200, data 19.343) 
Epoch 13 Iter 1000, train entropy gap 1.0478 bits (loss 20.391, data 19.343) 
Epoch 13 Iter 1200, train entropy gap 0.8220 bits (loss 20.165, data 19.343) 
Epoch 13 Iter 1400, train entropy gap 0.9282 bits (loss 20.272, data 19.343) 
Epoch 13 Iter 1600, train entropy gap 0.9974 bits (loss 20.341, data 19.343) 
Epoch 13 Iter 1800, train entropy gap 0.8581 bits (loss 20.202, data 19.343) 
Epoch 13 Iter 2000, train entropy gap 0.9200 bits (loss 20.263, data 19.343) 
Epoch 13 Iter 2200, train entropy gap 0.9182 bits (loss 20.262, data 19.343) 
Epoch 13 Iter 2400, train entropy gap 0.9125 bits (loss 20.256, data 19.343) 
Epoch 13 Iter 2600, train entropy gap 0.9947 bits (loss 20.338, data 19.343) 
Epoch 13 Iter 2800, train entropy gap 1.0980 bits (loss 20.441, data 19.343) 
Epoch 13 Iter 3000, train entropy gap 1.0639 bits (loss 20.407, data 19.343) 
Epoch 13 Iter 3200, train entropy gap 0.9951 bits (loss 20.339, data 19.343) 
Epoch 13 Iter 3400, train entropy gap 0.9909 bits (loss 20.334, data 19.343) 
epoch 13 train loss 14.0697 nats / 20.2983 bits
time since start: 439.5 secs
Epoch 14 Iter 0, train entropy gap 1.0874 bits (loss 20.431, data 19.343) 
Epoch 14 Iter 200, train entropy gap 0.8787 bits (loss 20.222, data 19.343) 
Epoch 14 Iter 400, train entropy gap 1.0505 bits (loss 20.394, data 19.343) 
Epoch 14 Iter 600, train entropy gap 0.8324 bits (loss 20.176, data 19.343) 
Epoch 14 Iter 800, train entropy gap 0.9195 bits (loss 20.263, data 19.343) 
Epoch 14 Iter 1000, train entropy gap 0.7906 bits (loss 20.134, data 19.343) 
Epoch 14 Iter 1200, train entropy gap 0.8861 bits (loss 20.230, data 19.343) 
Epoch 14 Iter 1400, train entropy gap 0.9233 bits (loss 20.267, data 19.343) 
Epoch 14 Iter 1600, train entropy gap 0.9431 bits (loss 20.287, data 19.343) 
Epoch 14 Iter 1800, train entropy gap 0.8724 bits (loss 20.216, data 19.343) 
Epoch 14 Iter 2000, train entropy gap 0.8872 bits (loss 20.231, data 19.343) 
Epoch 14 Iter 2200, train entropy gap 0.8542 bits (loss 20.198, data 19.343) 
Epoch 14 Iter 2400, train entropy gap 0.9224 bits (loss 20.266, data 19.343) 
Epoch 14 Iter 2600, train entropy gap 0.7810 bits (loss 20.124, data 19.343) 
Epoch 14 Iter 2800, train entropy gap 0.9677 bits (loss 20.311, data 19.343) 
Epoch 14 Iter 3000, train entropy gap 1.0292 bits (loss 20.373, data 19.343) 
Epoch 14 Iter 3200, train entropy gap 0.9684 bits (loss 20.312, data 19.343) 
Epoch 14 Iter 3400, train entropy gap 0.8650 bits (loss 20.208, data 19.343) 
epoch 14 train loss 14.0649 nats / 20.2914 bits
time since start: 471.1 secs
Epoch 15 Iter 0, train entropy gap 0.9115 bits (loss 20.255, data 19.343) 
Epoch 15 Iter 200, train entropy gap 0.8702 bits (loss 20.214, data 19.343) 
Epoch 15 Iter 400, train entropy gap 0.8524 bits (loss 20.196, data 19.343) 
Epoch 15 Iter 600, train entropy gap 1.0258 bits (loss 20.369, data 19.343) 
Epoch 15 Iter 800, train entropy gap 0.9104 bits (loss 20.254, data 19.343) 
Epoch 15 Iter 1000, train entropy gap 0.8870 bits (loss 20.230, data 19.343) 
Epoch 15 Iter 1200, train entropy gap 0.9315 bits (loss 20.275, data 19.343) 
Epoch 15 Iter 1400, train entropy gap 0.9547 bits (loss 20.298, data 19.343) 
Epoch 15 Iter 1600, train entropy gap 0.8257 bits (loss 20.169, data 19.343) 
Epoch 15 Iter 1800, train entropy gap 0.7606 bits (loss 20.104, data 19.343) 
Epoch 15 Iter 2000, train entropy gap 0.9763 bits (loss 20.320, data 19.343) 
Epoch 15 Iter 2200, train entropy gap 0.9544 bits (loss 20.298, data 19.343) 
Epoch 15 Iter 2400, train entropy gap 1.0996 bits (loss 20.443, data 19.343) 
Epoch 15 Iter 2600, train entropy gap 0.8717 bits (loss 20.215, data 19.343) 
Epoch 15 Iter 2800, train entropy gap 0.9481 bits (loss 20.292, data 19.343) 
Epoch 15 Iter 3000, train entropy gap 0.8966 bits (loss 20.240, data 19.343) 
Epoch 15 Iter 3200, train entropy gap 0.9905 bits (loss 20.334, data 19.343) 
Epoch 15 Iter 3400, train entropy gap 0.8244 bits (loss 20.168, data 19.343) 
epoch 15 train loss 14.0604 nats / 20.2848 bits
time since start: 502.2 secs
Epoch 16 Iter 0, train entropy gap 1.0855 bits (loss 20.429, data 19.343) 
Epoch 16 Iter 200, train entropy gap 0.9649 bits (loss 20.308, data 19.343) 
Epoch 16 Iter 400, train entropy gap 0.9246 bits (loss 20.268, data 19.343) 
Epoch 16 Iter 600, train entropy gap 0.8555 bits (loss 20.199, data 19.343) 
Epoch 16 Iter 800, train entropy gap 0.9116 bits (loss 20.255, data 19.343) 
Epoch 16 Iter 1000, train entropy gap 0.8715 bits (loss 20.215, data 19.343) 
Epoch 16 Iter 1200, train entropy gap 1.1147 bits (loss 20.458, data 19.343) 
Epoch 16 Iter 1400, train entropy gap 0.9661 bits (loss 20.310, data 19.343) 
Epoch 16 Iter 1600, train entropy gap 0.8683 bits (loss 20.212, data 19.343) 
Epoch 16 Iter 1800, train entropy gap 0.9076 bits (loss 20.251, data 19.343) 
Epoch 16 Iter 2000, train entropy gap 0.9347 bits (loss 20.278, data 19.343) 
Epoch 16 Iter 2200, train entropy gap 0.8685 bits (loss 20.212, data 19.343) 
Epoch 16 Iter 2400, train entropy gap 1.0619 bits (loss 20.405, data 19.343) 
Epoch 16 Iter 2600, train entropy gap 0.8920 bits (loss 20.235, data 19.343) 
Epoch 16 Iter 2800, train entropy gap 1.0105 bits (loss 20.354, data 19.343) 
Epoch 16 Iter 3000, train entropy gap 0.9589 bits (loss 20.302, data 19.343) 
Epoch 16 Iter 3200, train entropy gap 0.8505 bits (loss 20.194, data 19.343) 
Epoch 16 Iter 3400, train entropy gap 0.9200 bits (loss 20.263, data 19.343) 
epoch 16 train loss 14.0562 nats / 20.2788 bits
time since start: 533.5 secs
Epoch 17 Iter 0, train entropy gap 0.8992 bits (loss 20.243, data 19.343) 
Epoch 17 Iter 200, train entropy gap 0.9919 bits (loss 20.335, data 19.343) 
Epoch 17 Iter 400, train entropy gap 0.9428 bits (loss 20.286, data 19.343) 
Epoch 17 Iter 600, train entropy gap 0.8295 bits (loss 20.173, data 19.343) 
Epoch 17 Iter 800, train entropy gap 0.9661 bits (loss 20.310, data 19.343) 
Epoch 17 Iter 1000, train entropy gap 0.9065 bits (loss 20.250, data 19.343) 
Epoch 17 Iter 1200, train entropy gap 0.9332 bits (loss 20.277, data 19.343) 
Epoch 17 Iter 1400, train entropy gap 0.9459 bits (loss 20.289, data 19.343) 
Epoch 17 Iter 1600, train entropy gap 1.0702 bits (loss 20.414, data 19.343) 
Epoch 17 Iter 1800, train entropy gap 0.9972 bits (loss 20.341, data 19.343) 
Epoch 17 Iter 2000, train entropy gap 1.0238 bits (loss 20.367, data 19.343) 
Epoch 17 Iter 2200, train entropy gap 0.8524 bits (loss 20.196, data 19.343) 
Epoch 17 Iter 2400, train entropy gap 1.1235 bits (loss 20.467, data 19.343) 
Epoch 17 Iter 2600, train entropy gap 0.8779 bits (loss 20.221, data 19.343) 
Epoch 17 Iter 2800, train entropy gap 0.9722 bits (loss 20.316, data 19.343) 
Epoch 17 Iter 3000, train entropy gap 1.0812 bits (loss 20.425, data 19.343) 
Epoch 17 Iter 3200, train entropy gap 0.9656 bits (loss 20.309, data 19.343) 
Epoch 17 Iter 3400, train entropy gap 0.9205 bits (loss 20.264, data 19.343) 
epoch 17 train loss 14.0523 nats / 20.2732 bits
time since start: 564.6 secs
Epoch 18 Iter 0, train entropy gap 0.9182 bits (loss 20.262, data 19.343) 
Epoch 18 Iter 200, train entropy gap 0.9359 bits (loss 20.279, data 19.343) 
Epoch 18 Iter 400, train entropy gap 0.9360 bits (loss 20.280, data 19.343) 
Epoch 18 Iter 600, train entropy gap 0.8515 bits (loss 20.195, data 19.343) 
Epoch 18 Iter 800, train entropy gap 0.8349 bits (loss 20.178, data 19.343) 
Epoch 18 Iter 1000, train entropy gap 0.8483 bits (loss 20.192, data 19.343) 
Epoch 18 Iter 1200, train entropy gap 0.9279 bits (loss 20.271, data 19.343) 
Epoch 18 Iter 1400, train entropy gap 0.8883 bits (loss 20.232, data 19.343) 
Epoch 18 Iter 1600, train entropy gap 0.9241 bits (loss 20.268, data 19.343) 
Epoch 18 Iter 1800, train entropy gap 0.8599 bits (loss 20.203, data 19.343) 
Epoch 18 Iter 2000, train entropy gap 0.9464 bits (loss 20.290, data 19.343) 
Epoch 18 Iter 2200, train entropy gap 0.7546 bits (loss 20.098, data 19.343) 
Epoch 18 Iter 2400, train entropy gap 0.9968 bits (loss 20.340, data 19.343) 
Epoch 18 Iter 2600, train entropy gap 1.0152 bits (loss 20.359, data 19.343) 
Epoch 18 Iter 2800, train entropy gap 1.0190 bits (loss 20.362, data 19.343) 
Epoch 18 Iter 3000, train entropy gap 0.8460 bits (loss 20.189, data 19.343) 
Epoch 18 Iter 3200, train entropy gap 0.8897 bits (loss 20.233, data 19.343) 
Epoch 18 Iter 3400, train entropy gap 0.9774 bits (loss 20.321, data 19.343) 
epoch 18 train loss 14.0487 nats / 20.2680 bits
time since start: 595.9 secs
Epoch 19 Iter 0, train entropy gap 1.0521 bits (loss 20.396, data 19.343) 
Epoch 19 Iter 200, train entropy gap 0.8908 bits (loss 20.234, data 19.343) 
Epoch 19 Iter 400, train entropy gap 0.8899 bits (loss 20.233, data 19.343) 
Epoch 19 Iter 600, train entropy gap 0.9554 bits (loss 20.299, data 19.343) 
Epoch 19 Iter 800, train entropy gap 0.9116 bits (loss 20.255, data 19.343) 
Epoch 19 Iter 1000, train entropy gap 0.9763 bits (loss 20.320, data 19.343) 
Epoch 19 Iter 1200, train entropy gap 0.9747 bits (loss 20.318, data 19.343) 
Epoch 19 Iter 1400, train entropy gap 0.9409 bits (loss 20.284, data 19.343) 
Epoch 19 Iter 1600, train entropy gap 0.9737 bits (loss 20.317, data 19.343) 
Epoch 19 Iter 1800, train entropy gap 1.0555 bits (loss 20.399, data 19.343) 
Epoch 19 Iter 2000, train entropy gap 1.0588 bits (loss 20.402, data 19.343) 
Epoch 19 Iter 2200, train entropy gap 0.8282 bits (loss 20.172, data 19.343) 
Epoch 19 Iter 2400, train entropy gap 1.0686 bits (loss 20.412, data 19.343) 
Epoch 19 Iter 2600, train entropy gap 0.8034 bits (loss 20.147, data 19.343) 
Epoch 19 Iter 2800, train entropy gap 1.0865 bits (loss 20.430, data 19.343) 
Epoch 19 Iter 3000, train entropy gap 0.9901 bits (loss 20.334, data 19.343) 
Epoch 19 Iter 3200, train entropy gap 0.9000 bits (loss 20.243, data 19.343) 
Epoch 19 Iter 3400, train entropy gap 0.8698 bits (loss 20.213, data 19.343) 
epoch 19 train loss 14.0455 nats / 20.2633 bits
time since start: 627.1 secs
Training done; evaluating likelihood on full data:
Epoch None Iter 0, test loss 17.7882 nats / 25.6629 bits
Epoch None Iter 500, test loss 15.0439 nats / 21.7037 bits
Epoch None Iter 1000, test loss 13.2146 nats / 19.0646 bits
Epoch None Iter 1500, test loss 12.5689 nats / 18.1330 bits
Epoch None Iter 2000, test loss 13.4445 nats / 19.3963 bits
Epoch None Iter 2500, test loss 12.3775 nats / 17.8570 bits
Epoch None Iter 3000, test loss 15.4896 nats / 22.3468 bits
Epoch None Iter 3500, test loss 12.9563 nats / 18.6920 bits
Epoch None Iter 4000, test loss 12.5724 nats / 18.1381 bits
Epoch None Iter 4500, test loss 13.6909 nats / 19.7518 bits
Epoch None Iter 5000, test loss 12.7243 nats / 18.3573 bits
Epoch None Iter 5500, test loss 15.4453 nats / 22.2828 bits
Epoch None Iter 6000, test loss 14.0182 nats / 20.2239 bits
Epoch None Iter 6500, test loss 13.0352 nats / 18.8059 bits
Epoch None Iter 7000, test loss 17.0436 nats / 24.5887 bits
Saved to:
models/dmv-ofnan-6.0MB-model20.264-data19.343-made-resmade-hidden256_256_256_256_256-emb32-directIo-binaryInone_hotOut-inputNoEmbIfLeq-20epochs-seed0.pt
